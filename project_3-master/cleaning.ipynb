{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4252443a-4f98-4ceb-b0f4-1480f23abbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8dce565-c91a-4ac0-9e99-e1b6bd8df200",
   "metadata": {},
   "outputs": [],
   "source": [
    "futurology = pd.read_pickle('data/futurology.pkl')\n",
    "collapse = pd.read_pickle('data/collapse.pkl')\n",
    "complete_df = collapse.append(futurology, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41943c23-2d22-487f-905a-f166f799f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize, remove special characters and stopwords, lemmatize \n",
    "def cleaner (text):\n",
    "    sw = stopwords.words('english')\n",
    "    unneeded_punc = ['.', ',']\n",
    "    low = text.lower()\n",
    "    tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "    lem = WordNetLemmatizer()\n",
    "    tokens = tokenizer.tokenize(low)\n",
    "    \n",
    "    # I don't need any tiny fragments or extraneous punctuation and want to lemmatize it # also I'm seeing lots of links, which I don't want\n",
    "    tokens_lem = [lem.lemmatize(token) for token in tokens if len(token) >2]\n",
    "    tokens_clean = [token for token in tokens_lem if token not in sw]\n",
    "    tokens_stripped = [token.replace('.', '') for token in tokens_clean]\n",
    "    tokens_stripped = [token.replace('.', '') for token in tokens_stripped]\n",
    "    tokens_stripped = [token for token in tokens_stripped if token.find('http')==-1]\n",
    "    return tokens_stripped\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d44453f-6467-4719-bb52-233503feb6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SmartestNPC</td>\n",
       "      <td>1</td>\n",
       "      <td>[agree, first, idea,, medium, also, essence, p...</td>\n",
       "      <td>1633301435</td>\n",
       "      <td>collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Free-Layer-706</td>\n",
       "      <td>1</td>\n",
       "      <td>[amazing, think, close, came, 2020, election, ...</td>\n",
       "      <td>1633301403</td>\n",
       "      <td>collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrypticResponseMan</td>\n",
       "      <td>1</td>\n",
       "      <td>[caaaaaaaarl, that’s, wrong!, kill, people!]</td>\n",
       "      <td>1633301363</td>\n",
       "      <td>collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DocMoochal</td>\n",
       "      <td>1</td>\n",
       "      <td>[&amp;gt;will, significant, change, reaction/, con...</td>\n",
       "      <td>1633301337</td>\n",
       "      <td>collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CrypticResponseMan</td>\n",
       "      <td>1</td>\n",
       "      <td>[it’s, way, like, i’ll, never, get, bored]</td>\n",
       "      <td>1633301336</td>\n",
       "      <td>collapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>SnooOwls2295</td>\n",
       "      <td>1</td>\n",
       "      <td>[pretty, clear, wfh, crowd, worse, this, look,...</td>\n",
       "      <td>1633299326</td>\n",
       "      <td>futurology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>Browncoat_Loyalist</td>\n",
       "      <td>1</td>\n",
       "      <td>[please, keep, slide, *mostly*, disposable!]</td>\n",
       "      <td>1633299293</td>\n",
       "      <td>futurology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>greenSixx</td>\n",
       "      <td>1</td>\n",
       "      <td>[no,, skipped, known, years]</td>\n",
       "      <td>1633299275</td>\n",
       "      <td>futurology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>noglovesincleantrash</td>\n",
       "      <td>1</td>\n",
       "      <td>[[recycling, nuclear]</td>\n",
       "      <td>1633299268</td>\n",
       "      <td>futurology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>greenSixx</td>\n",
       "      <td>1</td>\n",
       "      <td>[practice, more, turn, work, computer, turn, w...</td>\n",
       "      <td>1633299256</td>\n",
       "      <td>futurology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4094 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    author  score  \\\n",
       "0              SmartestNPC      1   \n",
       "1           Free-Layer-706      1   \n",
       "2       CrypticResponseMan      1   \n",
       "3               DocMoochal      1   \n",
       "4       CrypticResponseMan      1   \n",
       "...                    ...    ...   \n",
       "4089          SnooOwls2295      1   \n",
       "4090    Browncoat_Loyalist      1   \n",
       "4091             greenSixx      1   \n",
       "4092  noglovesincleantrash      1   \n",
       "4093             greenSixx      1   \n",
       "\n",
       "                                                   body     created  \\\n",
       "0     [agree, first, idea,, medium, also, essence, p...  1633301435   \n",
       "1     [amazing, think, close, came, 2020, election, ...  1633301403   \n",
       "2          [caaaaaaaarl, that’s, wrong!, kill, people!]  1633301363   \n",
       "3     [&gt;will, significant, change, reaction/, con...  1633301337   \n",
       "4            [it’s, way, like, i’ll, never, get, bored]  1633301336   \n",
       "...                                                 ...         ...   \n",
       "4089  [pretty, clear, wfh, crowd, worse, this, look,...  1633299326   \n",
       "4090       [please, keep, slide, *mostly*, disposable!]  1633299293   \n",
       "4091                       [no,, skipped, known, years]  1633299275   \n",
       "4092                              [[recycling, nuclear]  1633299268   \n",
       "4093  [practice, more, turn, work, computer, turn, w...  1633299256   \n",
       "\n",
       "       subreddit  \n",
       "0       collapse  \n",
       "1       collapse  \n",
       "2       collapse  \n",
       "3       collapse  \n",
       "4       collapse  \n",
       "...          ...  \n",
       "4089  futurology  \n",
       "4090  futurology  \n",
       "4091  futurology  \n",
       "4092  futurology  \n",
       "4093  futurology  \n",
       "\n",
       "[4094 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens = complete_df.copy()\n",
    "df_tokens['body'] = df_tokens['body'].apply(lambda x : cleaner(x))\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69633ddc-0bdb-4512-b7af-419ee43aae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens.to_pickle('data/df_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f98a2-6261-4144-82ae-47671086201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_untoken = df_tokens.copy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
